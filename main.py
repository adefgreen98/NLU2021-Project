# -*- coding: utf-8 -*-
"""preprocess_embed.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aMxmDzFuFuUJ5Hr23pZCT-pTRjpfWZ15

# General Notes
-  Considering entities as multiple tokens **not stuck together**, each token must be recognized independently from the others
- Scanning sentence left-right: since entities should be ordered we are avoiding assigning same entity to duplicate tokens

# Dependencies
"""

from itertools import product

from utils import *
from nlp_init import get_preprocessor

def test(net, test_sentence, padder=None):
    print("------------- TEST -------------")
    test_inference = net.run_inference(test_sentence)
    print(f"""Lengths: Sent = {len(test_sentence.split())} | Inf = {len(test_inference)}""")
    # print(*list(zip(test_sentence.split(), test_inference)), sep='\n')
    print(*list(zip(padder(test_sentence), test_inference)), sep='\n')


def simple_inference():
    nlp = get_preprocessor()
    dataset = get_dataset('ATIS\\train.json', nlp)
    labels = dataset.get_labels()
    net = get_model('gru', labels, nlp.vocab.vectors_length, 50, device='cpu')
    test_sentence = "i want a morning flight from boston to chicago"
    inference_embedder_fn = lambda sent: dataset.preprocess_single_sentence(sent, dataset.get_max_sent_length())
    net.set_embedder(inference_embedder_fn)
    padder = lambda x: dataset._preprocess_test_sentence(x, pad_len=30)
    test(net, test_sentence, padder)



"""# Main loop"""

def main(**kwargs):
    nlp = get_preprocessor()

    dataset = get_dataset(kwargs["train_path"], nlp)
    labels = dataset.get_labels()
    train_set, valid_set = split_dataset(dataset, valid_ratio=kwargs["valid_ratio"])
    
    train_dataloader = get_dataloader(train_set, batch_size=kwargs["batch_size"], shuffle=False)
    eval_dataloader = get_dataloader(valid_set, batch_size=kwargs["batch_size"])
    # test_dataloader = get_dataloader(get_dataset(kwargs["test_path"], nlp), batch_size=1)
    
    net = get_model(kwargs["model"], labels, nlp.vocab.vectors_length, kwargs["hidden_size"], device=get_device())
    
    optimizer = get_optimizer(net, kwargs["learning_rate"], kwargs["optimizer"])

    loss_fn = get_loss(kwargs["loss"])
    
    train(kwargs["nr_epochs"], net, train_dataloader, optimizer, loss_fn, valid_dl=eval_dataloader)
    
    # Testing phase
    test_sentence = "i want a morning flight from boston to chicago"

    # building embedder to have just 1 argument, as needed by the model
    inference_embedder_fn = lambda sent: dataset.preprocess_single_sentence(sent, dataset.get_max_sent_length())
    net.set_embedder(inference_embedder_fn)

    padder = lambda x: dataset._preprocess_test_sentence(x, pad_len=30)
    test(net, test_sentence, padder)



def produce_configurations(params):
    param_names = list(params.keys())
    configurations = product(*list(params.values()))

    for cfg in configurations:
        yield {k: v for k, v in zip(param_names, cfg)}



parameters = {
    "valid_ratio": [0.1],
    "batch_size": [32],
    "model": ["lstm"],
    "loss": ["cross_entropy"],
    "optimizer": ["sgd"],
    "hidden_size": [50],
    "learning_rate": [3e-4],
    "nr_epochs": [20],
}


if __name__ == '__main__':

    for cfg in produce_configurations(parameters):
        print("---> Configuration: <---", *[str(k) + ": " + str(v) for k,v in cfg.items()], sep='\n')
        main(train_path = "ATIS/train.json", test_path = "ATIS/test.json", save_path = "", **cfg)



